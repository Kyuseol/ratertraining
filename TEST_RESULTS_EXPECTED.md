# 전문가 5명 MFRM 캘리브레이션 테스트 예상 결과

## 테스트 설정

### 시뮬레이션 파라미터

| 전문가 | 엄격성 (진짜 값) | 설명 |
|--------|-----------------|------|
| expert_1 | 0.00 | 중립 |
| expert_2 | -0.50 | 엄격 (낮은 점수 경향) |
| expert_3 | +0.20 | 약간 관대 |
| expert_4 | +0.50 | 관대 (높은 점수 경향) |
| expert_5 | -0.20 | 약간 엄격 |

### 데이터 규모

- **전문가**: 5명
- **에세이**: 24편
- **평가요소**: 8개
- **총 관측치**: 5 × 24 × 8 = **960개**

---

## 예상 결과

### 1. 점수 분포

```
1점 (미흡): ~25%
2점 (보통): ~48%
3점 (우수): ~27%
```

### 2. 전문가별 평균 점수

| 전문가 | 진짜 엄격성 | 예상 평균 점수 | 설명 |
|--------|------------|---------------|------|
| expert_2 | -0.50 | ~2.3 | 가장 엄격 → 낮은 점수 |
| expert_5 | -0.20 | ~2.1 | |
| expert_1 | 0.00 | ~2.0 | 중립 |
| expert_3 | +0.20 | ~1.9 | |
| expert_4 | +0.50 | ~1.7 | 가장 관대 → 높은 점수 |

**참고**: MFRM에서 엄격한 평가자(낮은 점수)는 **양의 엄격성 값**으로 추정됩니다.

### 3. 캘리브레이션 품질 지표

| 지표 | 예상 값 | 기준 |
|------|---------|------|
| 분리 신뢰도 | 0.70-0.90 | ≥ 0.70 권장 |
| 수렴 | TRUE | |
| 에세이 난이도 범위 | -2.0 ~ +2.0 logit | |

### 4. 추정 정확도

시뮬레이션 데이터의 경우:

- **에세이 난이도**: 추정-진짜 상관계수 ≥ 0.80 예상
- **전문가 엄격성**: 추정-진짜 상관계수 ≥ 0.70 예상

---

## 테스트 방법

### 방법 1: Node.js 테스트 스크립트

```bash
# R Backend가 실행 중인 상태에서
cd C:\project\mfrm-project
node test_calibration.js
```

### 방법 2: Frontend UI

1. `npm start`로 프론트엔드 실행
2. 관리자 로그인
3. `/admin/experts`에서 전문가 5명 등록
4. `/admin/expert-rating`에서 채점 (각 전문가가 모든 에세이 채점)
5. `/admin/calibration`에서 "MFRM 캘리브레이션 실행"

### 방법 3: R 직접 실행

```r
# R Console에서
setwd("C:/project/mfrm-project/backend")
source("test_expert_calibration.R")
```

---

## 핵심 포인트

1. **960개 관측치**로 진정한 MFRM 분석 가능
2. **전문가 엄격성**과 **에세이 난이도** 동시 추정
3. **분리 신뢰도**로 추정 품질 확인
4. 추정된 난이도를 앵커 에세이에 **고정** → 이후 교사 채점 분석에 활용

---

## 다음 단계

캘리브레이션 완료 후:

1. **활성화** 버튼 클릭 → 난이도 값이 에세이에 고정됨
2. 일반 **교사 채점** 시작
3. 교사 채점 데이터로 **MFRM 분석** → 교사 엄격성 추정
4. 앵커 에세이 난이도는 고정되어 있으므로 **일관된 척도** 유지

